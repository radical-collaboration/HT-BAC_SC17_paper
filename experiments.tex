\subsection{Performance Evaluation}

\subsubsection{A. Experiment Setup}



We perform scalability experiments on Blue Waters HPC resource-- a 13.3 petaFLOPS Cray at NCSA and University of Illinois, with 32 Interlago cores/50 GB RAM per node, Cray Gemini, Lustre shared file system. 

We ran the HT-BAC workflow with null workloads where the task did no work (/bin/sleep 0), and NAMD workload launched using MPI. The number of replicas ranged from 8 to 128 running concurrently using 8 cores per replica. Figure X shows the overheads as measured by RP for the null workload. 

The null workload demonstrates the core overhead which is the time-to-completion (TTC) as measured by RADICAL Pilot. 

We characterize the scalability of both the null 

We characterize the scalability of the ESMACS protocol HT-BAC workload using Ensemble Toolkit by performing weak scaling tests for ensemble of pipelines pattern. We validate the overhead by measuring the overhead of bin sleep0. Figure X presents the results of our weak scaling experiments. We keep the number of cores designated for the pilot size consistent with the number of cores for the simulation. We vary the number of pipelines between 8 to 128, where each pipeline receives 8 cores per stage. 

NAMD logs - corrolate the overhead for each pipeline makes the system invariant to the worload. Remaining will be RP overhead 


NAMD log files (utime) demonstrates time-to-execution (Tx) 




(ii) Archer a Cray XC30 at UK NSS with 12-core Intel Ivy Bridge 64 GB RAM, Lustre shared FS. 

Add in: what are the overheads



1) Weak scaling test: 
