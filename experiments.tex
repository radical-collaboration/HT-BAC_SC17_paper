\subsection{Performance Evaluation}

\subsubsection{A. Experiment Setup}

We perform scalability experiments on Blue Waters HPC resource-- a 13.3 petaFLOPS Cray at NCSA and University of Illinois, with 32 Interlago cores/50 GB RAM per node, Cray Gemini, Lustre shared file system. 

We characterize the weak scalability of the ESMACS protocol HT-BAC workflow using Ensemble Toolkit. We ran the HT-BAC workflow with null workloads where the task did no work (/bin/sleep 0), and NAMD workload launched using MPI.The size of the workload is varied in proportion with the amount of resources such that all tasks are concurrently executed at all times. The number of replicas ranged from 8 to 128 running concurrently using 8 cores per replica. Figure X shows the overheads as measured by RP for the null workload. The null workload demonstrates the core overhead which is the time-to-completion (TTC) as measured by RADICAL Pilot. For the NAMD workload, figure Y shows scalability results for the overheads and the simulation execution time which corresponds to the time taken by all the simulations to complete. All the experiments use Ensemble Toolkit version 0.47. 

NAMD logs - corrolate the overhead for each pipeline makes the system invariant to the worload. Remaining will be RP overhead. NAMD log files (utime) demonstrates time-to-execution (Tx) 

Add in: what are the overheads and how HT-BAC is amendable to run on other HPC resources including NSS Archer 



